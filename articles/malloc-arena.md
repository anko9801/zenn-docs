---
title: "malloc.c ã‚’èª­ã‚€ (arena)"
emoji: "ğŸ·"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: []
published: false
---

`malloc` é–¢æ•°ã§ãƒ’ãƒ¼ãƒ—é ˜åŸŸã«ã‚ã‚‹ãƒ¡ãƒ¢ãƒªã‚’ç¢ºä¿ã—ã¦ãã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¿”ã™ã€‚ `free` é–¢æ•°ã¯ãã®ãƒã‚¤ãƒ³ã‚¿ã®ãƒ¡ãƒ¢ãƒªã‚’é–‹æ”¾ã—ã¦ãã‚Œã‚‹ã€‚

```c
void *malloc(size_t size);
void free(void *ptr);
```

ã“ã‚Œã‚‰ã®é–¢æ•°ãŒå†…éƒ¨ã§ã©ã®ã‚ˆã†ã«å‡¦ç†ã•ã‚Œã‚‹ã®ã‹ã‚’èª¿ã¹ã¦ã„ãã€‚

[**æœ€æ–°ã® malloc.c ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰**](https://elixir.bootlin.com/glibc/latest/source/malloc/malloc.c) ã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ã€‚

ä»Šå›ã¯ã‚¢ãƒªãƒ¼ãƒŠã®å‡¦ç†ã‚’ä¸­å¿ƒã«èª¿ã¹ã¦ã„ãã€‚

glibc ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ v2.38 ã§ã™ã€‚

## ã‚¢ãƒªãƒ¼ãƒŠã¨ã¯

```c
struct malloc_state
{
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
};
```

main arena ã¯æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒ«ãƒ¼ãƒ—ã«ãªã£ã¦ã„ã¾ã™ã€‚

```c
/* There are several instances of this struct ("arenas") in this
   malloc.  If you are adapting this malloc in a way that does NOT use
   a static or mmapped malloc_state, you MUST explicitly zero-fill it
   before using. This malloc relies on the property that malloc_state
   is initialized to all zeroes (as is true of C statics).  */

static struct malloc_state main_arena =
{
  .mutex = _LIBC_LOCK_INITIALIZER,
  .next = &main_arena,
  .attached_threads = 1
};
```

### Bins

bins ã¯åŒæ–¹å‘ã® free list ã®å…ˆé ­ãƒ»æœ«å°¾ã‚’æ ¼ç´ã™ã‚‹é…åˆ—ã§ã™ã€‚

å…¨éƒ¨ã§ 128 å€‹ã® bins ãŒã‚ã£ã¦ã€ã‚ã‚‹ã‚µã‚¤ã‚ºç¯„å›²ã”ã¨ã«ä¿æŒã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿéš›ä¸Šã€å°ã•ã„ã»ã©é »ç¹ã«ã€å¤§ãã„ã»ã©ç¨€ã« malloc ã•ã‚Œã‚‹ã“ã¨ãŒçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ç‚ºã€ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«é€£ã‚Œã¦æŒ‡æ•°çš„ã«é–“éš”ã‚’å¤§ããã™ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

largebins ã¯ã‚µã‚¤ã‚ºé †ã«ä¿æŒã•ã‚Œã€smallbins ã¯ã™ã¹ã¦åŒã˜ã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ãªã®ã§é †åºä»˜ã‘ã—ãªãã¦ã‚‚æœ€é©ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¾ã™ã€‚

ã¾ãŸ bins ã«ãŠã„ã¦é€£çµã—ã¦ã‹ã‚‰æŒ¿å…¥ã•ã‚Œã‚‹ç‚ºã€ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆå†…ã®ãƒãƒ£ãƒ³ã‚¯åŒå£«ã¯ç‰©ç†çš„ã«æ¥ã—ãªã„ã¨ã„ã†æ€§è³ªã‚’æŒã£ã¦ã„ã¾ã™ã€‚

åŒã˜ã‚µã‚¤ã‚ºã®ãƒãƒ£ãƒ³ã‚¯ã¯ã€æœ€è¿‘è§£æ”¾ã•ã‚ŒãŸã‚‚ã®ã‚’å…ˆé ­ã«ãƒªãƒ³ã‚¯ã•ã‚Œã€å‰²ã‚Šå½“ã¦ã¯å¾Œã‚ã‹ã‚‰è¡Œã‚ã‚Œã¾ã™ã€‚ ã“ã®çµæœã€LRU (FIFO) å‰²ã‚Šå½“ã¦é †ã¨ãªã‚Šã€å„ãƒãƒ£ãƒ³ã‚¯ã«éš£æ¥ã™ã‚‹è§£æ”¾ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã¨é€£çµã•ã‚Œã‚‹æ©Ÿä¼šãŒå‡ç­‰ã«ä¸ãˆã‚‰ã‚Œã‚‹å‚¾å‘ãŒã‚ã‚‹ãŸã‚ã€ç©ºããƒãƒ£ãƒ³ã‚¯ãŒå¤§ãããªã‚Šã€æ–­ç‰‡åŒ–ãŒå°‘ãªããªã‚Šã¾ã™ã€‚

ã“ã“ã«ã‚‚ 1 ã¤ãƒˆãƒªãƒƒã‚¯ãŒã‚ã‚Šã€å„ bin ã®ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ `malloc_chunk` ã® `fd` `bk` ã¨ã—ã¦æ‰±ã†ã“ã¨ã§ç°¡å˜ã«ä½¿ç”¨ã§ãã¾ã™ã€‚

```c
typedef struct malloc_chunk *mbinptr;

/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &((m)->bins[((i) - 1) * 2]))			      \
             - offsetof (struct malloc_chunk, fd))

/* analog of ++bin */
#define next_bin(b)  ((mbinptr) ((char *) (b) + (sizeof (mchunkptr) << 1)))

/* Reminders about list directionality within bins */
#define first(b)     ((b)->fd)
#define last(b)      ((b)->bk)

/*
   Indexing

    Bins for sizes < 512 bytes contain chunks of all the same size, spaced
    8 bytes apart. Larger bins are approximately logarithmically spaced:

    64 bins of size       8
    32 bins of size      64
    16 bins of size     512
     8 bins of size    4096
     4 bins of size   32768
     2 bins of size  262144
     1 bin  of size what's left

    There is actually a little bit of slop in the numbers in bin_index
    for the sake of speed. This makes no difference elsewhere.

    The bins top out around 1MB because we expect to service large
    requests via mmap.

    Bin 0 does not exist.  Bin 1 is the unordered list; if that would be
    a valid chunk size the small bins are bumped up one.
 */

#define NBINS             128
#define NSMALLBINS         64
#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
#define SMALLBIN_CORRECTION (MALLOC_ALIGNMENT > CHUNK_HDR_SZ)
#define MIN_LARGE_SIZE    ((NSMALLBINS - SMALLBIN_CORRECTION) * SMALLBIN_WIDTH)

#define in_smallbin_range(sz)  \
  ((unsigned long) (sz) < (unsigned long) MIN_LARGE_SIZE)

#define smallbin_index(sz) \
  ((SMALLBIN_WIDTH == 16 ? (((unsigned) (sz)) >> 4) : (((unsigned) (sz)) >> 3))\
   + SMALLBIN_CORRECTION)

#define largebin_index_32(sz)                                                \
  (((((unsigned long) (sz)) >> 6) <= 38) ?  56 + (((unsigned long) (sz)) >> 6) :\
   ((((unsigned long) (sz)) >> 9) <= 20) ?  91 + (((unsigned long) (sz)) >> 9) :\
   ((((unsigned long) (sz)) >> 12) <= 10) ? 110 + (((unsigned long) (sz)) >> 12) :\
   ((((unsigned long) (sz)) >> 15) <= 4) ? 119 + (((unsigned long) (sz)) >> 15) :\
   ((((unsigned long) (sz)) >> 18) <= 2) ? 124 + (((unsigned long) (sz)) >> 18) :\
   126)

#define largebin_index_32_big(sz)                                            \
  (((((unsigned long) (sz)) >> 6) <= 45) ?  49 + (((unsigned long) (sz)) >> 6) :\
   ((((unsigned long) (sz)) >> 9) <= 20) ?  91 + (((unsigned long) (sz)) >> 9) :\
   ((((unsigned long) (sz)) >> 12) <= 10) ? 110 + (((unsigned long) (sz)) >> 12) :\
   ((((unsigned long) (sz)) >> 15) <= 4) ? 119 + (((unsigned long) (sz)) >> 15) :\
   ((((unsigned long) (sz)) >> 18) <= 2) ? 124 + (((unsigned long) (sz)) >> 18) :\
   126)

// XXX It remains to be seen whether it is good to keep the widths of
// XXX the buckets the same or whether it should be scaled by a factor
// XXX of two as well.
#define largebin_index_64(sz)                                                \
  (((((unsigned long) (sz)) >> 6) <= 48) ?  48 + (((unsigned long) (sz)) >> 6) :\
   ((((unsigned long) (sz)) >> 9) <= 20) ?  91 + (((unsigned long) (sz)) >> 9) :\
   ((((unsigned long) (sz)) >> 12) <= 10) ? 110 + (((unsigned long) (sz)) >> 12) :\
   ((((unsigned long) (sz)) >> 15) <= 4) ? 119 + (((unsigned long) (sz)) >> 15) :\
   ((((unsigned long) (sz)) >> 18) <= 2) ? 124 + (((unsigned long) (sz)) >> 18) :\
   126)

#define largebin_index(sz) \
  (SIZE_SZ == 8 ? largebin_index_64 (sz)                                     \
   : MALLOC_ALIGNMENT == 16 ? largebin_index_32_big (sz)                     \
   : largebin_index_32 (sz))

#define bin_index(sz) \
  ((in_smallbin_range (sz)) ? smallbin_index (sz) : largebin_index (sz))

/* The otherwise unindexable 1-bin is used to hold unsorted chunks. */
#define unsorted_chunks(M)          (bin_at (M, 1))
```

ã“ã‚Œã‚’èª­ã‚€ã¨ 64bits ç’°å¢ƒ ã«ãŠã„ã¦ `unsortedbin` `smallbins` `largebins` ã¯æ¬¡ã®ã‚ˆã†ã«è¡¨ç¾ã•ã‚Œã‚‹ã€‚

| åå‰ | ç¯„å›² | ç¯„å›² (ãƒã‚¤ãƒˆè¡¨ç¤º) | é–“éš” | å€‹æ•° | `bin_at(n)` |
| --- | --- | --- | --- | :-: | --- |
| unsortedbin | 0x20 ~ | ã™ã¹ã¦ | infinity | 1 | 1 |
| smallbins | 0x20 ~ 0x3F0 | 1KB æœªæº€ | 0x10 | 62 | 2 ~ 63 |
| largebins | 0x400 ~ 0xC30 | 1KB ä»¥ä¸Š 3KB æœªæº€ | 0x40 | 35 | 64 ~ 96 |
| largebins | 0xC40 ~ 0x29F0 | 3KB ä»¥ä¸Š 12KB æœªæº€ | 0x200 | 15 | 97 ~ 111 |
| largebins | 0x3000 ~ 0xAFF0 | 12KB ä»¥ä¸Š 44KB æœªæº€ | 0x1000 | 9 | 112 ~ 120 |
| largebins | 0xB000 ~ 0x27FF0 | 44KB ä»¥ä¸Š 160KB æœªæº€ | 0x8000 | 3 | 121 ~ 123 |
| largebins | 0x28000 ~ 0xBFFF0 | 160KB ä»¥ä¸Š 768KB æœªæº€ | 0x40000 | 2 | 124 ~ 125 |
| largebins | 0xC0000 ~  | 768KB ä»¥ä¸Š | infinity | 1 | 126 |

ã¡ãªã¿ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸Šã’ã‚‹ç‚ºã«æŒ‡æ•°çš„ã«é–“éš”ãŒä¸Šæ˜‡ã›ãšã€å¢ƒç•Œä»˜è¿‘ã¯ç¶ºéº—ã«åˆ†ã‘ã‚‰ã‚Œã¦ã¾ã›ã‚“ã€‚

1MB ä»¥ä¸Šã¯ mmap ã‚’ç”¨ã„ã¾ã™ã€‚

### Binmap

malloc ã«ãŠã„ã¦å¤§é‡ã® bin ã®æ¤œç´¢ã‚’è£œã†ç‚ºã«å„ç“¶ãŒç©ºã§ã‚ã‚‹ã‹ã©ã†ã‹ã‚’è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãƒ“ãƒƒãƒˆãƒ™ã‚¯ã‚¿ã§ã™ã€‚

ã“ã®ãƒ“ãƒƒãƒˆã¯ãƒ“ãƒ³ãŒç©ºã«ãªã‚‹ã¨ã™ãã«ã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ã®ã§ã¯ãªãã€mallocã§ã®æ¢ç´¢ä¸­ã«ãƒ“ãƒ³ãŒç©ºã§ã‚ã‚‹ã“ã¨ã«æ°—ä»˜ã„ãŸæ™‚ã«ã‚¯ãƒªã‚¢ã•ã‚Œã¾ã™ã€‚

```c
/* Conservatively use 32 bits per map word, even if on 64bit system */
#define BINMAPSHIFT      5
#define BITSPERMAP       (1U << BINMAPSHIFT)
#define BINMAPSIZE       (NBINS / BITSPERMAP)

#define idx2block(i)     ((i) >> BINMAPSHIFT)
#define idx2bit(i)       ((1U << ((i) & ((1U << BINMAPSHIFT) - 1))))

#define mark_bin(m, i)    ((m)->binmap[idx2block (i)] |= idx2bit (i))
#define unmark_bin(m, i)  ((m)->binmap[idx2block (i)] &= ~(idx2bit (i)))
#define get_binmap(m, i)  ((m)->binmap[idx2block (i)] & idx2bit (i))

static void *
_int_malloc (mstate av, size_t bytes)
{
  for (;; )
    {
      ...
       /*
         Search for a chunk by scanning bins, starting with next largest
         bin. This search is strictly by best-fit; i.e., the smallest
         (with ties going to approximately the least recently used) chunk
         that fits is selected.

         The bitmap avoids needing to check that most blocks are nonempty.
         The particular case of skipping all bins during warm-up phases
         when no chunks have been returned yet is faster than it might look.
       */

      ++idx;
      bin = bin_at (av, idx);
      block = idx2block (idx);
      map = av->binmap[block];
      bit = idx2bit (idx);

      for (;; )
        {
          /* Skip rest of block if there are no more set bits in this block.  */
          if (bit > map || bit == 0)
            {
              do
                {
                  if (++block >= BINMAPSIZE) /* out of bins */
                    goto use_top;
                }
              while ((map = av->binmap[block]) == 0);

              bin = bin_at (av, (block << BINMAPSHIFT));
              bit = 1;
            }

          /* Advance to bin with set bit. There must be one. */
          while ((bit & map) == 0)
            {
              bin = next_bin (bin);
              bit <<= 1;
              assert (bit != 0);
            }

          /* Inspect the bin. It is likely to be non-empty */
          victim = last (bin);

          /*  If a false alarm (empty bin), clear the bit. */
          if (victim == bin)
            {
              av->binmap[block] = map &= ~bit; /* Write through */
              bin = next_bin (bin);
              bit <<= 1;
            }

          else
            {
              ...
            }
        }
    }
}
```

ã“ã‚Œã‚’èª­ã‚€ã¨ 32 bit ã®ãƒ•ãƒ©ã‚°ã‚’ 4 block ç”¨æ„ã—ã¦ã™ã¹ã¦ã® bin ã®ãƒ•ãƒ©ã‚°ã‚’è¡¨ç¾ã—ã¦ã„ã¦ã€

### top
 The top-most available chunk (i.e., the one bordering the end of available memory) is treated specially. It is never included in any bin, is used only if no other chunk is available, and is released back to the system if it is very large (see M_TRIM_THRESHOLD).  Because top initially points to its own bin with initial zero size, thus forcing extension on the first malloc request, we avoid having any special code in malloc to check whether it even exists yet. But we still need to do so when getting memory from system, so we make initial_top treat the bin as a legal but unusable chunk during the interval between initialization and the first call to sysmalloc. (This is somewhat delicate, since it relies on the 2 preceding words to be zero during this interval as well.)

```c
/* Conveniently, the unsorted bin can be used as dummy top on first call */
#define initial_top(M)              (unsorted_chunks (M))
```

### last_remainder
smallbins
```c
              /* Split */
              else
                {
                  remainder = chunk_at_offset (victim, nb);

                  /* We cannot assume the unsorted list is empty and therefore
                     have to perform a complete insert here.  */
                  bck = unsorted_chunks (av);
                  fwd = bck->fd;
		  if (__glibc_unlikely (fwd->bk != bck))
		    malloc_printerr ("malloc(): corrupted unsorted chunks 2");
                  remainder->bk = bck;
                  remainder->fd = fwd;
                  bck->fd = remainder;
                  fwd->bk = remainder;

                  /* advertise as last remainder */
                  if (in_smallbin_range (nb))
                    av->last_remainder = remainder;
                  if (!in_smallbin_range (remainder_size))
                    {
                      remainder->fd_nextsize = NULL;
                      remainder->bk_nextsize = NULL;
                    }
                  set_head (victim, nb | PREV_INUSE |
                            (av != &main_arena ? NON_MAIN_ARENA : 0));
                  set_head (remainder, remainder_size | PREV_INUSE);
                  set_foot (remainder, remainder_size);
                }
```

ã“ã‚Œã‚’èª­ã‚€ã¨ smallbins

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```c
struct malloc_par
{
  /* Tunable parameters */
  unsigned long trim_threshold;
  INTERNAL_SIZE_T top_pad;
  INTERNAL_SIZE_T mmap_threshold;
  INTERNAL_SIZE_T arena_test;
  INTERNAL_SIZE_T arena_max;

  /* Transparent Large Page support.  */
  INTERNAL_SIZE_T thp_pagesize;
  /* A value different than 0 means to align mmap allocation to hp_pagesize
     add hp_flags on flags.  */
  INTERNAL_SIZE_T hp_pagesize;
  int hp_flags;

  /* Memory map support */
  int n_mmaps;
  int n_mmaps_max;
  int max_n_mmaps;
  /* the mmap_threshold is dynamic, until the user sets
     it manually, at which point we need to disable any
     dynamic behavior. */
  int no_dyn_threshold;

  /* Statistics */
  INTERNAL_SIZE_T mmapped_mem;
  INTERNAL_SIZE_T max_mmapped_mem;

  /* First address handed out by MORECORE/sbrk.  */
  char *sbrk_base;

#if USE_TCACHE
  /* Maximum number of buckets to use.  */
  size_t tcache_bins;
  size_t tcache_max_bytes;
  /* Maximum number of chunks in each bucket.  */
  size_t tcache_count;
  /* Maximum number of chunks to remove from the unsorted list, which
     aren't used to prefill the cache.  */
  size_t tcache_unsorted_limit;
#endif
};
```

```c
/* There is only one instance of the malloc parameters.  */

static struct malloc_par mp_ =
{
  .top_pad = DEFAULT_TOP_PAD,
  .n_mmaps_max = DEFAULT_MMAP_MAX,
  .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
  .trim_threshold = DEFAULT_TRIM_THRESHOLD,
#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
  .arena_test = NARENAS_FROM_NCORES (1)
#if USE_TCACHE
  ,
  .tcache_count = TCACHE_FILL_COUNT,
  .tcache_bins = TCACHE_MAX_BINS,
  .tcache_max_bytes = tidx2usize (TCACHE_MAX_BINS-1),
  .tcache_unsorted_limit = 0 /* No limit.  */
#endif
};
```

```c
/*
   Initialize a malloc_state struct.

   This is called from ptmalloc_init () or from _int_new_arena ()
   when creating a new arena.
 */

static void
malloc_init_state (mstate av)
{
  int i;
  mbinptr bin;

  /* Establish circular links for normal bins */
  for (i = 1; i < NBINS; ++i)
    {
      bin = bin_at (av, i);
      bin->fd = bin->bk = bin;
    }

#if MORECORE_CONTIGUOUS
  if (av != &main_arena)
#endif
  set_noncontiguous (av);
  if (av == &main_arena)
    set_max_fast (DEFAULT_MXFAST);
  atomic_store_relaxed (&av->have_fastchunks, false);

  av->top = initial_top (av);
}
```

