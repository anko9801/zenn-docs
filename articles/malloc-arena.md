---
title: "malloc.c „ÇíË™≠„ÇÄ (arena)"
emoji: "üê∑"
type: "tech" # tech: ÊäÄË°ìË®ò‰∫ã / idea: „Ç¢„Ç§„Éá„Ç¢
topics: ["CTF", "pwn", "Linux"]
published: false
---

`malloc()` „Åß„Éí„Éº„ÉóÈ†òÂüü„Å´„ÅÇ„Çã„É°„É¢„É™„ÇíÁ¢∫‰øù„Åó„Å¶„Åù„ÅÆ„Éù„Ç§„É≥„Çø„ÇíËøî„Åó„ÄÅ`free()` „ÅØ„Åù„ÅÆ„Éù„Ç§„É≥„Çø„ÅÆ„É°„É¢„É™„ÇíÈñãÊîæ„Åó„Å¶„Åè„Çå„Åæ„Åô„ÄÇ

```c
void *malloc(size_t size);
void free(void *ptr);
```

„Åì„ÅÆ„Ç∑„É™„Éº„Ç∫„Åß„ÅØ„Åì„Çå„Çâ„ÅÆÈñ¢Êï∞„ÅåÂÜÖÈÉ®„Åß„Å©„ÅÆ„Çà„ÅÜ„Å´Âá¶ÁêÜ„Åï„Çå„Çã„ÅÆ„Åã„ÇíË™ø„Åπ„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇ

‰ªäÂõû„ÅØ„Ç¢„É™„Éº„Éä„ÅÆÂá¶ÁêÜ„Çí‰∏≠ÂøÉ„Å´Ë™ø„Åπ„Å¶„ÅÑ„Åç„Åæ„Åô„ÄÇ

„Ç∑„É™„Éº„Ç∫„ÅØ malloc.c „ÇíÈ°åÊùê„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„Åì„Åì„Åß„ÅØ arena.c „Çí‰∏ª„Å´Ë™≠„ÇÄ„Åì„Å®„Å´„Å™„Çä„Åæ„Åô„ÄÇ

„Åì„Åì„ÅßÊâ±„ÅÜ glibc „ÅÆ„Éê„Éº„Ç∏„Éß„É≥„ÅØ v2.38 „Åß„Åô„ÄÇ„Åæ„Åü glibc „ÅÆ„ÇΩ„Éº„Çπ„Ç≥„Éº„Éâ„ÅØ„Éñ„É©„Ç¶„Ç∂‰∏ä„ÅßË™≠„ÇÄ„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ

- https://elixir.bootlin.com/glibc/latest/source/malloc/malloc.c
- https://codebrowser.dev/glibc/glibc/malloc/arena.c.html

## „Ç¢„É™„Éº„Éä„Å®„ÅØ

`malloc_state` ÊßãÈÄ†‰Ωì„Å®„Åó„Å¶ÂÆöÁæ©„Åï„Çå„Çã„ÄÇ

```c:malloc_state#L
struct malloc_state
{
  __libc_lock_define (, mutex);     // arena „Å∏„ÅÆ„Ç¢„ÇØ„Çª„Çπ„Çí serialize „Åô„Çã
  int flags;                        // „Éí„Éº„Éó„É°„É¢„É™„ÅåÈÄ£Á∂ö„Åß„ÅÇ„Çã„Åã

  int have_fastchunks;              // fastbins „ÅåÁ©∫„Åß„ÅØ„Å™„ÅÑ„Åì„Å®„ÇíË°®„ÅôÁúüÂÅΩÂÄ§
  mfastbinptr fastbinsY[NFASTBINS]; // fastbins

  mchunkptr top;                    // „Éí„Éº„ÉóÈ†òÂüü„ÅÆÊúÄÂæå„Å´„ÅÇ„ÇãÊú™‰ΩøÁî®„ÅÆÂ§ß„Åç„Å™„ÉÅ„É£„É≥„ÇØ
  mchunkptr last_remainder;         // ÂàÜÂâ≤„Åó„Å¶Á¢∫‰øù„Åó„ÅüÈöõ„Å´‰Ωô„Å£„ÅüÈ†òÂüü„ÅÆÊúÄÊñ∞„ÅÆ„ÉÅ„É£„É≥„ÇØ

  mchunkptr bins[NBINS * 2 - 2];    // unsortedbin smallbins largebins „ÅÆÂÖàÈ†≠„ÉªÊú´Â∞æ
  unsigned int binmap[BINMAPSIZE];  // „Åì„Çå„Çâ„ÇíÁ¥†Êó©„ÅèË¶ã„Å§„Åë„ÇãÁÇ∫„Å´‰Ωø„Çè„Çå„Çã„Éì„ÉÉ„Éà„Éô„ÇØ„Çø

  struct malloc_state *next;        // arena „ÅÆÂçòÊñπÂêë„É™„Çπ„Éà
  struct malloc_state *next_free;   // ‰Ωø„Çè„Çå„Å¶„ÅÑ„Å™„ÅÑ arena „ÅÆÂçòÊñπÂêë„É™„Çπ„Éà
  INTERNAL_SIZE_T attached_threads; // arena „Å´„Ç¢„ÇØ„Çª„Çπ„Åó„Å¶„ÅÑ„Çã„Çπ„É¨„ÉÉ„Éâ„ÅÆÊï∞

  INTERNAL_SIZE_T system_mem;       // arena „Å´„Çà„Å£„Å¶ÁèæÂú®Á¢∫‰øù„Åï„Çå„Å¶„ÅÑ„Çã„É°„É¢„É™„ÅÆÂêàË®àÂÄ§
  INTERNAL_SIZE_T max_system_mem;   // system_mem „ÅÆÊúÄÂ§ßÂÄ§
};
```

main arena „ÅØÊ¨°„ÅÆ„Çà„ÅÜ„Å´ÂÆöÁæ©„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
„É´„Éº„Éó„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ

```c
static struct malloc_state main_arena =
{
  .mutex = _LIBC_LOCK_INITIALIZER,
  .next = &main_arena,
  .attached_threads = 1
};
```

## „Éë„É©„É°„Éº„Çø
```c
struct malloc_par
{
  /* Tunable parameters */
  unsigned long trim_threshold;
  INTERNAL_SIZE_T top_pad;
  INTERNAL_SIZE_T mmap_threshold;
  INTERNAL_SIZE_T arena_test;
  INTERNAL_SIZE_T arena_max;

  /* Transparent Large Page support.  */
  INTERNAL_SIZE_T thp_pagesize;
  /* A value different than 0 means to align mmap allocation to hp_pagesize
     add hp_flags on flags.  */
  INTERNAL_SIZE_T hp_pagesize;
  int hp_flags;

  /* Memory map support */
  int n_mmaps;
  int n_mmaps_max;
  int max_n_mmaps;
  /* the mmap_threshold is dynamic, until the user sets
     it manually, at which point we need to disable any
     dynamic behavior. */
  int no_dyn_threshold;

  /* Statistics */
  INTERNAL_SIZE_T mmapped_mem;
  INTERNAL_SIZE_T max_mmapped_mem;

  /* First address handed out by MORECORE/sbrk.  */
  char *sbrk_base;

#if USE_TCACHE
  /* Maximum number of buckets to use.  */
  size_t tcache_bins;
  size_t tcache_max_bytes;
  /* Maximum number of chunks in each bucket.  */
  size_t tcache_count;
  /* Maximum number of chunks to remove from the unsorted list, which
     aren't used to prefill the cache.  */
  size_t tcache_unsorted_limit;
#endif
};
```

```c
/* There is only one instance of the malloc parameters.  */

static struct malloc_par mp_ =
{
  .top_pad = DEFAULT_TOP_PAD,
  .n_mmaps_max = DEFAULT_MMAP_MAX,
  .mmap_threshold = DEFAULT_MMAP_THRESHOLD,
  .trim_threshold = DEFAULT_TRIM_THRESHOLD,
#define NARENAS_FROM_NCORES(n) ((n) * (sizeof (long) == 4 ? 2 : 8))
  .arena_test = NARENAS_FROM_NCORES (1)
#if USE_TCACHE
  ,
  .tcache_count = TCACHE_FILL_COUNT,
  .tcache_bins = TCACHE_MAX_BINS,
  .tcache_max_bytes = tidx2usize (TCACHE_MAX_BINS-1),
  .tcache_unsorted_limit = 0 /* No limit.  */
#endif
};
```

## ÂêÑ„Éï„Ç£„Éº„É´„Éâ„Å´Èñ¢ÈÄ£„Åô„ÇãÂá¶ÁêÜ

### top
 The top-most available chunk (i.e., the one bordering the end of available memory) is treated specially. It is never included in any bin, is used only if no other chunk is available, and is released back to the system if it is very large (see M_TRIM_THRESHOLD).  Because top initially points to its own bin with initial zero size, thus forcing extension on the first malloc request, we avoid having any special code in malloc to check whether it even exists yet. But we still need to do so when getting memory from system, so we make initial_top treat the bin as a legal but unusable chunk during the interval between initialization and the first call to sysmalloc. (This is somewhat delicate, since it relies on the 2 preceding words to be zero during this interval as well.)

```c
/* Conveniently, the unsorted bin can be used as dummy top on first call */
#define initial_top(M)              (unsorted_chunks (M))
```

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;


### last_remainder
smallbins
```c
              /* Split */
              else
                {
                  remainder = chunk_at_offset (victim, nb);

                  /* We cannot assume the unsorted list is empty and therefore
                     have to perform a complete insert here.  */
                  bck = unsorted_chunks (av);
                  fwd = bck->fd;
		  if (__glibc_unlikely (fwd->bk != bck))
		    malloc_printerr ("malloc(): corrupted unsorted chunks 2");
                  remainder->bk = bck;
                  remainder->fd = fwd;
                  bck->fd = remainder;
                  fwd->bk = remainder;

                  /* advertise as last remainder */
                  if (in_smallbin_range (nb))
                    av->last_remainder = remainder;
                  if (!in_smallbin_range (remainder_size))
                    {
                      remainder->fd_nextsize = NULL;
                      remainder->bk_nextsize = NULL;
                    }
                  set_head (victim, nb | PREV_INUSE |
                            (av != &main_arena ? NON_MAIN_ARENA : 0));
                  set_head (remainder, remainder_size | PREV_INUSE);
                  set_foot (remainder, remainder_size);
                }
```

„Åì„Çå„ÇíË™≠„ÇÄ„Å® smallbins

```c
/*
   Initialize a malloc_state struct.

   This is called from ptmalloc_init () or from _int_new_arena ()
   when creating a new arena.
 */

static void
malloc_init_state (mstate av)
{
  int i;
  mbinptr bin;

  /* Establish circular links for normal bins */
  for (i = 1; i < NBINS; ++i)
    {
      bin = bin_at (av, i);
      bin->fd = bin->bk = bin;
    }

#if MORECORE_CONTIGUOUS
  if (av != &main_arena)
#endif
  set_noncontiguous (av);
  if (av == &main_arena)
    set_max_fast (DEFAULT_MXFAST);
  atomic_store_relaxed (&av->have_fastchunks, false);

  av->top = initial_top (av);
}
```

